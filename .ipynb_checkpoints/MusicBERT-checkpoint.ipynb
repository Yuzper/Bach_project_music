{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74333f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f73c84a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fairseq in c:\\users\\jespe\\anaconda3\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: cython in c:\\users\\jespe\\anaconda3\\lib\\site-packages (from fairseq) (0.29.28)\n",
      "Requirement already satisfied: regex in c:\\users\\jespe\\anaconda3\\lib\\site-packages (from fairseq) (2022.3.15)\n",
      "Requirement already satisfied: sacrebleu>=1.4.12 in c:\\users\\jespe\\anaconda3\\lib\\site-packages (from fairseq) (2.3.1)\n",
      "Requirement already satisfied: cffi in c:\\users\\jespe\\anaconda3\\lib\\site-packages (from fairseq) (1.15.0)\n",
      "Requirement already satisfied: omegaconf<2.1 in c:\\users\\jespe\\anaconda3\\lib\\site-packages (from fairseq) (2.0.6)\n",
      "Requirement already satisfied: torch in c:\\users\\jespe\\anaconda3\\lib\\site-packages (from fairseq) (1.13.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jespe\\anaconda3\\lib\\site-packages (from fairseq) (4.64.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\jespe\\anaconda3\\lib\\site-packages (from fairseq) (1.21.5)\n",
      "Requirement already satisfied: torchaudio>=0.8.0 in c:\\users\\jespe\\anaconda3\\lib\\site-packages (from fairseq) (0.13.1)\n",
      "Requirement already satisfied: hydra-core<1.1,>=1.0.7 in c:\\users\\jespe\\anaconda3\\lib\\site-packages (from fairseq) (1.0.7)\n",
      "Requirement already satisfied: bitarray in c:\\users\\jespe\\anaconda3\\lib\\site-packages (from fairseq) (2.4.1)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in c:\\users\\jespe\\anaconda3\\lib\\site-packages (from hydra-core<1.1,>=1.0.7->fairseq) (4.8)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jespe\\anaconda3\\lib\\site-packages (from omegaconf<2.1->fairseq) (4.1.1)\n",
      "Requirement already satisfied: PyYAML>=5.1.* in c:\\users\\jespe\\anaconda3\\lib\\site-packages (from omegaconf<2.1->fairseq) (6.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\jespe\\anaconda3\\lib\\site-packages (from sacrebleu>=1.4.12->fairseq) (0.8.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\jespe\\anaconda3\\lib\\site-packages (from sacrebleu>=1.4.12->fairseq) (0.4.4)\n",
      "Requirement already satisfied: portalocker in c:\\users\\jespe\\anaconda3\\lib\\site-packages (from sacrebleu>=1.4.12->fairseq) (2.7.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\jespe\\anaconda3\\lib\\site-packages (from sacrebleu>=1.4.12->fairseq) (4.8.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\jespe\\anaconda3\\lib\\site-packages (from cffi->fairseq) (2.21)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\jespe\\anaconda3\\lib\\site-packages (from portalocker->sacrebleu>=1.4.12->fairseq) (302)\n"
     ]
    }
   ],
   "source": [
    "#!pip install fairseq\n",
    "#!pip install transformers\n",
    "#!pip install fairseq.models.roberta\n",
    "!pip install fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dfc765a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TransformerSentenceEncoder' from 'fairseq.models.roberta' (C:\\Users\\Jespe\\Anaconda3\\lib\\site-packages\\fairseq\\models\\roberta\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (MaskTokensDataset,\n\u001b[0;32m      7\u001b[0m                           LanguagePairDataset,\n\u001b[0;32m      8\u001b[0m                           PrependTokenDataset,\n\u001b[0;32m      9\u001b[0m                           data_utils)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_model, register_model_architecture\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mroberta\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformerSentenceEncoder, RobertaEncoder, RobertaModel\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_task\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msentence_prediction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentencePredictionTask\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TransformerSentenceEncoder' from 'fairseq.models.roberta' (C:\\Users\\Jespe\\Anaconda3\\lib\\site-packages\\fairseq\\models\\roberta\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import fairseq.tasks.sentence_prediction\n",
    "import fairseq.tasks.masked_lm\n",
    "from fairseq import metrics\n",
    "from fairseq.criterions import register_criterion\n",
    "from fairseq.criterions.sentence_prediction import SentencePredictionCriterion\n",
    "from fairseq.data import (MaskTokensDataset,\n",
    "                          LanguagePairDataset,\n",
    "                          PrependTokenDataset,\n",
    "                          data_utils)\n",
    "from fairseq.models import register_model, register_model_architecture\n",
    "#from fairseq.models.roberta import TransformerSentenceEncoder, RobertaEncoder, RobertaModel\n",
    "from fairseq.tasks import register_task\n",
    "from fairseq.tasks.sentence_prediction import SentencePredictionTask\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sklearn.metrics\n",
    "from functools import lru_cache\n",
    "from typing import Optional, Tuple\n",
    "import numpy as np\n",
    "import math\n",
    "import logging\n",
    "import os\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca48c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "disable_cp = 'disable_cp' in os.environ\n",
    "print('disable_cp =', disable_cp)\n",
    "mask_strategy = os.environ['mask_strategy'].split(\n",
    "    '+') if 'mask_strategy' in os.environ else ['bar']\n",
    "print('mask_strategy =', mask_strategy)\n",
    "assert all(item in ['element', 'compound', 'bar'] for item in mask_strategy)\n",
    "convert_encoding = os.environ['convert_encoding'] if 'convert_encoding' in os.environ else 'OCTMIDI'\n",
    "print('convert_encoding =', convert_encoding)\n",
    "crop_length = int(os.environ['crop_length']\n",
    "                  ) if 'crop_length' in os.environ else None\n",
    "print('crop_length =', crop_length)  # of compound tokens\n",
    "max_bars = 256\n",
    "max_instruments = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f525a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_task(\"sentence_prediction_multilabel\")\n",
    "class MusicBERTSentencePredictionMultilabelTask(SentencePredictionTask):\n",
    "    def load_dataset(self, split, combine=False, **kwargs):\n",
    "        split_path = os.path.join(self.args.data, 'input0', split)\n",
    "        input0 = data_utils.load_indexed_dataset(\n",
    "            split_path,\n",
    "            self.source_dictionary,\n",
    "            self.args.dataset_impl,\n",
    "            combine=combine,\n",
    "        )\n",
    "        if self.args.init_token is not None:\n",
    "            input0 = OctupleTokenDataset(input0)\n",
    "        src_dataset = input0\n",
    "        labels, label_lengths = [], []\n",
    "        with open(os.path.join(self.args.data, 'label', split+\".label\")) as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                line = line.split()\n",
    "                label = [self.label_dictionary.index(item) for item in line]\n",
    "\n",
    "                if(len(label) < self.args.num_classes):\n",
    "                    label = label + \\\n",
    "                        [self.label_dictionary.index(\n",
    "                            '<pad>')]*(self.args.num_classes-len(label))\n",
    "\n",
    "                label = label[:self.args.num_classes]\n",
    "\n",
    "                label = torch.tensor(label)\n",
    "                labels.append(label)\n",
    "                label_lengths.append(len(label))\n",
    "        assert len(src_dataset) == len(labels)\n",
    "        self.datasets[split] = LanguagePairDataset(\n",
    "            src=src_dataset,\n",
    "            src_sizes=src_dataset.sizes,\n",
    "            src_dict=self.label_dictionary,\n",
    "            tgt=labels,\n",
    "            tgt_sizes=torch.tensor(label_lengths),\n",
    "            tgt_dict=self.label_dictionary,\n",
    "            left_pad_source=False,\n",
    "            input_feeding=False,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e17064",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_criterion(\"sentence_prediction_multilabel\")\n",
    "class MusicBERTSentencePredictionMultilabelCriterion(SentencePredictionCriterion):\n",
    "    def forward(self, model, sample, reduce=True):\n",
    "        assert (\n",
    "            hasattr(model, \"classification_heads\")\n",
    "            and self.classification_head_name in model.classification_heads\n",
    "        ), \"model must provide sentence classification head for --criterion=sentence_prediction\"\n",
    "        logits, _ = model(\n",
    "            **sample[\"net_input\"],\n",
    "            features_only=True,\n",
    "            classification_head_name=self.classification_head_name,\n",
    "        )\n",
    "        targets = model.get_targets(sample, [logits])\n",
    "        targets = F.one_hot(targets.long(), num_classes=logits.size()[-1]+4)\n",
    "        targets = targets.sum(dim=1)\n",
    "        targets = targets[:, 4:]\n",
    "        loss = F.binary_cross_entropy_with_logits(\n",
    "            logits, targets.float(), reduction='sum')\n",
    "        sample_size = logits.size()[0]\n",
    "        logging_output = {\n",
    "            \"loss\": loss.data,\n",
    "            \"ntokens\": sample_size * logits.size()[1],\n",
    "            \"nsentences\": sample_size,\n",
    "            \"sample_size\": sample_size,\n",
    "        }\n",
    "        preds = F.relu(torch.sign(logits))\n",
    "        logging_output[\"ncorrect\"] = sample_size - \\\n",
    "            torch.sign((preds != targets).sum(dim=1)).sum().data\n",
    "        logging_output[\"y_true\"] = targets.detach().cpu().numpy()\n",
    "        logging_output[\"y_pred\"] = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "        return loss, sample_size, logging_output\n",
    "\n",
    "    @staticmethod\n",
    "    def reduce_metrics(logging_outputs) -> None:\n",
    "        loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n",
    "        ntokens = sum(log.get(\"ntokens\", 0) for log in logging_outputs)\n",
    "        nsentences = sum(log.get(\"nsentences\", 0) for log in logging_outputs)\n",
    "        sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n",
    "        metrics.log_scalar(\n",
    "            \"loss\", loss_sum / sample_size / math.log(2), sample_size, round=3\n",
    "        )\n",
    "        if sample_size != ntokens:\n",
    "            metrics.log_scalar(\n",
    "                \"nll_loss\", loss_sum / ntokens / math.log(2), ntokens, round=3\n",
    "            )\n",
    "        if len(logging_outputs) > 0 and \"ncorrect\" in logging_outputs[0]:\n",
    "            ncorrect = sum(log.get(\"ncorrect\", 0) for log in logging_outputs)\n",
    "            metrics.log_scalar(\n",
    "                \"accuracy\", 100.0 * ncorrect / nsentences, nsentences, round=1\n",
    "            )\n",
    "        if len(logging_outputs) > 0 and \"y_pred\" in logging_outputs[0]:\n",
    "            y_pred = np.vstack(tuple(log.get(\"y_pred\")\n",
    "                                     for log in logging_outputs if \"y_pred\" in log))\n",
    "            y_true = np.vstack(tuple(log.get(\"y_true\")\n",
    "                                     for log in logging_outputs if \"y_true\" in log))\n",
    "            for score in [\"roc_auc_score\", \"f1_score\"]:\n",
    "                for average in [\"macro\", \"micro\", \"weighted\", \"samples\"]:\n",
    "                    try:\n",
    "                        y_score = np.round(\n",
    "                            y_pred) if score == \"f1_score\" else y_pred\n",
    "                        kwargs = {\n",
    "                            \"zero_division\": 0} if score == \"f1_score\" else dict()\n",
    "                        result = sklearn.metrics.__dict__[score](\n",
    "                            y_true, y_score, average=average, **kwargs)\n",
    "                        metrics.log_scalar(\n",
    "                            \"{}_{}\".format(score, average), result)\n",
    "                    except BaseException as e:\n",
    "                        metrics.log_scalar(\n",
    "                            \"{}_{}\".format(score, average), None)\n",
    "\n",
    "    @staticmethod\n",
    "    def logging_outputs_can_be_summed() -> bool:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OctupleMaskTokensDataset(MaskTokensDataset):\n",
    "    @lru_cache(maxsize=8)\n",
    "    def __getitem__(self, index: int):\n",
    "        with data_utils.numpy_seed(self.seed, self.epoch, index):\n",
    "            item = self.dataset[index]\n",
    "            sz = len(item)\n",
    "            assert (\n",
    "                self.mask_idx not in item\n",
    "            ), \"Dataset contains mask_idx (={}), this is not expected!\".format(\n",
    "                self.mask_idx,\n",
    "            )\n",
    "            assert not self.mask_whole_words, 'mask whole words not supported for cp'\n",
    "\n",
    "            def generate_mask(sz, prob):\n",
    "                mask_n = np.random.rand(sz)\n",
    "                mask_s = np.zeros(sz, dtype=np.int8)\n",
    "                mask_s += mask_n < prob * \\\n",
    "                    (self.random_token_prob)  # 3 -> random\n",
    "                mask_s += mask_n < prob * \\\n",
    "                    (self.random_token_prob +\n",
    "                     self.leave_unmasked_prob)  # 2 -> original\n",
    "                mask_s += mask_n < prob * 1.00  # 1 -> [mask]\n",
    "                return mask_s\n",
    "            mask_prob = self.mask_prob\n",
    "            mask = np.zeros_like(item, dtype=np.int8)\n",
    "            # mask bos eos tokens (compound)\n",
    "            mask[:8] = np.repeat(generate_mask(1, mask_prob), 8)\n",
    "            # mask bos eos tokens (compound)\n",
    "            mask[-8:] = np.repeat(generate_mask(1, mask_prob), 8)\n",
    "            strategy = np.random.choice(mask_strategy)\n",
    "            if strategy == 'element':  # element level mask\n",
    "                mask[8: -8] = np.repeat(generate_mask(sz -\n",
    "                                                      2 * 8, mask_prob), 1)\n",
    "            if strategy == 'compound':  # compound token level mask\n",
    "                mask[8: -8] = np.repeat(generate_mask(sz //\n",
    "                                                      8 - 2, mask_prob), 8)\n",
    "            if strategy == 'bar':  # bar level mask\n",
    "                mask[8: -8] = generate_mask((max_bars * max_instruments + len(self.vocab)) * 8, mask_prob).reshape(-1, 8)[\n",
    "                    ((item[8: -8: 8] - 4) * max_instruments) + (item[8 + 2: -8 + 2: 8] - 4)].flatten()\n",
    "            if self.return_masked_tokens:\n",
    "                new_item = item.numpy()[:]\n",
    "                new_item[mask == 0] = self.pad_idx\n",
    "                return torch.from_numpy(new_item)\n",
    "            masked_item = np.random.choice(len(self.vocab), sz)\n",
    "            set_original = np.isin(mask, [0, 2])\n",
    "            masked_item[set_original] = item[set_original]\n",
    "            set_mask = np.isin(mask, [1])\n",
    "            masked_item[set_mask] = self.mask_idx\n",
    "            return torch.from_numpy(masked_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ce41c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OctupleEncoder(TransformerSentenceEncoder):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tpu = False\n",
    "        embedding_dim = kwargs['embedding_dim']\n",
    "        if not disable_cp:\n",
    "            self.downsampling = nn.Sequential(\n",
    "                nn.Linear(embedding_dim * 8, embedding_dim))\n",
    "            self.upsampling = nn.Sequential(\n",
    "                nn.Linear(embedding_dim, embedding_dim * 8))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tokens: torch.Tensor,\n",
    "        segment_labels: torch.Tensor = None,\n",
    "        last_state_only: bool = False,\n",
    "        positions: Optional[torch.Tensor] = None,\n",
    "        token_embeddings: Optional[torch.Tensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        ratio = 1 if disable_cp else 8\n",
    "        if not disable_cp:\n",
    "            assert tokens.shape[1] % ratio == 0, 'token sequences length should be multiple of ' + str(\n",
    "                ratio) + ' for compound mode'\n",
    "            assert last_state_only, 'hidden states not available for compound mode'\n",
    "            assert positions is None, 'custom positions is not supported for compound mode'\n",
    "            assert token_embeddings is None, 'custom token embeddings is not supported for compound mode'\n",
    "            assert segment_labels is None, 'segment embedding not supported for compound mode'\n",
    "        padding_mask = tokens[:, ::ratio].eq(self.padding_idx)\n",
    "        if not self.traceable and not self.tpu and not padding_mask.any():\n",
    "            padding_mask = None\n",
    "        if token_embeddings is not None:\n",
    "            x = token_embeddings\n",
    "        else:\n",
    "            x = self.embed_tokens(tokens)\n",
    "        if not disable_cp:\n",
    "            x = self.downsampling(x.view(x.shape[0], x.shape[1] // ratio, -1))\n",
    "        if self.embed_scale is not None:\n",
    "            x = x * self.embed_scale\n",
    "        if self.embed_positions is not None:\n",
    "            x = x + \\\n",
    "                self.embed_positions(tokens[:, ::ratio], positions=positions)\n",
    "        if self.segment_embeddings is not None and segment_labels is not None:\n",
    "            x = x + self.segment_embeddings(segment_labels)\n",
    "        if self.quant_noise is not None:\n",
    "            x = self.quant_noise(x)\n",
    "        if self.emb_layer_norm is not None:\n",
    "            x = self.emb_layer_norm(x)\n",
    "        x = self.dropout_module(x)\n",
    "        if padding_mask is not None:\n",
    "            x = x * (1 - padding_mask.unsqueeze(-1).type_as(x))\n",
    "        x = x.transpose(0, 1)\n",
    "        inner_states = []\n",
    "        if not last_state_only:\n",
    "            inner_states.append(x)\n",
    "        for layer in self.layers:\n",
    "            x, _ = layer(x, self_attn_padding_mask=padding_mask)\n",
    "            if not last_state_only:\n",
    "                inner_states.append(x)\n",
    "        if not disable_cp:\n",
    "            x = x.transpose(0, 1)\n",
    "            x = self.upsampling(x).view(x.shape[0], x.shape[1] * ratio, -1)\n",
    "            x = x.transpose(0, 1)\n",
    "        sentence_rep = x[0, :, :]\n",
    "        if last_state_only:\n",
    "            inner_states = [x]\n",
    "        if self.traceable:\n",
    "            return torch.stack(inner_states), sentence_rep\n",
    "        else:\n",
    "            return inner_states, sentence_rep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddfc1304",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RobertaEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMusicBERTEncoder\u001b[39;00m(\u001b[43mRobertaEncoder\u001b[49m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args, dictionary):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(args, dictionary)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RobertaEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "class MusicBERTEncoder(RobertaEncoder):\n",
    "    def __init__(self, args, dictionary):\n",
    "        super().__init__(args, dictionary)\n",
    "        self.sentence_encoder = OctupleEncoder(\n",
    "            padding_idx=dictionary.pad(),\n",
    "            vocab_size=len(dictionary),\n",
    "            num_encoder_layers=args.encoder_layers,\n",
    "            embedding_dim=args.encoder_embed_dim,\n",
    "            ffn_embedding_dim=args.encoder_ffn_embed_dim,\n",
    "            num_attention_heads=args.encoder_attention_heads,\n",
    "            dropout=args.dropout,\n",
    "            attention_dropout=args.attention_dropout,\n",
    "            activation_dropout=args.activation_dropout,\n",
    "            layerdrop=args.encoder_layerdrop,\n",
    "            max_seq_len=args.max_positions,\n",
    "            num_segments=0,\n",
    "            encoder_normalize_before=True,\n",
    "            apply_bert_init=True,\n",
    "            activation_fn=args.activation_fn,\n",
    "            q_noise=args.quant_noise_pq,\n",
    "            qn_block_size=args.quant_noise_pq_block_size,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70699e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@register_model(\"musicbert\")\n",
    "class MusicBERTModel(RobertaModel):\n",
    "    @classmethod\n",
    "    def build_model(cls, args, task):\n",
    "        base_architecture(args)\n",
    "        if not hasattr(args, \"max_positions\"):\n",
    "            args.max_positions = args.tokens_per_sample\n",
    "        encoder = MusicBERTEncoder(args, task.source_dictionary)\n",
    "        return cls(args, encoder)\n",
    "\n",
    "\n",
    "@register_model_architecture(\"musicbert\", \"musicbert\")\n",
    "def base_architecture(args):\n",
    "    args.encoder_layers = getattr(args, \"encoder_layers\", 12)\n",
    "    args.encoder_embed_dim = getattr(args, \"encoder_embed_dim\", 768)\n",
    "    args.encoder_ffn_embed_dim = getattr(args, \"encoder_ffn_embed_dim\", 3072)\n",
    "    args.encoder_attention_heads = getattr(args, \"encoder_attention_heads\", 12)\n",
    "    args.activation_fn = getattr(args, \"activation_fn\", \"gelu\")\n",
    "    args.pooler_activation_fn = getattr(args, \"pooler_activation_fn\", \"tanh\")\n",
    "    args.dropout = getattr(args, \"dropout\", 0.1)\n",
    "    args.attention_dropout = getattr(args, \"attention_dropout\", 0.1)\n",
    "    args.activation_dropout = getattr(args, \"activation_dropout\", 0.0)\n",
    "    args.pooler_dropout = getattr(args, \"pooler_dropout\", 0.0)\n",
    "    args.encoder_layers_to_keep = getattr(args, \"encoder_layers_to_keep\", None)\n",
    "    args.encoder_layerdrop = getattr(args, \"encoder_layerdrop\", 0.0)\n",
    "    args.untie_weights_roberta = getattr(args, \"untie_weights_roberta\", False)\n",
    "    args.spectral_norm_classification_head = getattr(\n",
    "        args, \"spectral_norm_classification_head\", False\n",
    "    )\n",
    "\n",
    "\n",
    "@register_model_architecture(\"musicbert\", \"musicbert_base\")\n",
    "def musicbert_base_architecture(args):\n",
    "    base_architecture(args)\n",
    "\n",
    "\n",
    "@register_model_architecture(\"musicbert\", \"musicbert_large\")\n",
    "def musicbert_large_architecture(args):\n",
    "    args.encoder_layers = getattr(args, \"encoder_layers\", 24)\n",
    "    args.encoder_embed_dim = getattr(args, \"encoder_embed_dim\", 1024)\n",
    "    args.encoder_ffn_embed_dim = getattr(args, \"encoder_ffn_embed_dim\", 4096)\n",
    "    args.encoder_attention_heads = getattr(args, \"encoder_attention_heads\", 16)\n",
    "    base_architecture(args)\n",
    "\n",
    "\n",
    "@register_model_architecture(\"musicbert\", \"musicbert_medium\")\n",
    "def musicbert_medium_architecture(args):\n",
    "    args.encoder_layers = getattr(args, \"encoder_layers\", 8)\n",
    "    args.encoder_embed_dim = getattr(args, \"encoder_embed_dim\", 512)\n",
    "    args.encoder_ffn_embed_dim = getattr(args, \"encoder_ffn_embed_dim\", 2048)\n",
    "    args.encoder_attention_heads = getattr(args, \"encoder_attention_heads\", 8)\n",
    "    base_architecture(args)\n",
    "\n",
    "\n",
    "@register_model_architecture(\"musicbert\", \"musicbert_small\")\n",
    "def musicbert_small_architecture(args):\n",
    "    args.encoder_layers = getattr(args, \"encoder_layers\", 4)\n",
    "    args.encoder_embed_dim = getattr(args, \"encoder_embed_dim\", 512)\n",
    "    args.encoder_ffn_embed_dim = getattr(args, \"encoder_ffn_embed_dim\", 2048)\n",
    "    args.encoder_attention_heads = getattr(args, \"encoder_attention_heads\", 8)\n",
    "    base_architecture(args)\n",
    "\n",
    "\n",
    "@register_model_architecture(\"musicbert\", \"musicbert_mini\")\n",
    "def musicbert_mini_architecture(args):\n",
    "    args.encoder_layers = getattr(args, \"encoder_layers\", 4)\n",
    "    args.encoder_embed_dim = getattr(args, \"encoder_embed_dim\", 256)\n",
    "    args.encoder_ffn_embed_dim = getattr(args, \"encoder_ffn_embed_dim\", 1024)\n",
    "    args.encoder_attention_heads = getattr(args, \"encoder_attention_heads\", 4)\n",
    "    base_architecture(args)\n",
    "\n",
    "\n",
    "@register_model_architecture(\"musicbert\", \"musicbert_tiny\")\n",
    "def musicbert_tiny_architecture(args):\n",
    "    args.encoder_layers = getattr(args, \"encoder_layers\", 2)\n",
    "    args.encoder_embed_dim = getattr(args, \"encoder_embed_dim\", 128)\n",
    "    args.encoder_ffn_embed_dim = getattr(args, \"encoder_ffn_embed_dim\", 512)\n",
    "    args.encoder_attention_heads = getattr(args, \"encoder_attention_heads\", 2)\n",
    "    base_architecture(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6204fc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OctupleTokenDataset(PrependTokenDataset):\n",
    "    def adaptor(self, e):\n",
    "        prev_bar = None\n",
    "        prev_pos = None\n",
    "        prev_prog = None\n",
    "        new_e = []\n",
    "        for i in e:\n",
    "            if prev_bar != i[0]:\n",
    "                prev_bar = i[0]\n",
    "                prev_pos = None\n",
    "                new_e.append((i[0], None, None, None, None, None, i[6], None))\n",
    "            if prev_pos != i[1]:\n",
    "                prev_pos = i[1]\n",
    "                prev_prog = None\n",
    "                new_e.append((None, i[1], None, None, None, None, None, i[7]))\n",
    "            if prev_prog != i[2]:\n",
    "                prev_prog = i[2]\n",
    "                new_e.append((None, None, i[2], None, None, None, None, None))\n",
    "            if True:\n",
    "                new_e.append((None, None, None, i[3], i[4], i[5], None, None))\n",
    "        return new_e\n",
    "\n",
    "    def convert(self, item):\n",
    "        encoding = item[8: -8].tolist()\n",
    "        encoding = list(tuple(encoding[i: i + 8])\n",
    "                        for i in range(0, len(encoding), 8))\n",
    "        encoding = self.adaptor(encoding)\n",
    "        if convert_encoding == 'CP':\n",
    "            encoding = list(3 if j is None else j for i in encoding for j in i)[\n",
    "                :crop_length * 8]\n",
    "        elif convert_encoding == 'REMI':\n",
    "            encoding = list(j for i in encoding for j in i if j is not None)[\n",
    "                :crop_length]\n",
    "        else:\n",
    "            assert False, 'Unknown encoding format'\n",
    "        bos = 0\n",
    "        eos = 2\n",
    "        encoding = ([bos] * 8) + encoding + ([eos] * 8)\n",
    "        return torch.tensor(encoding)\n",
    "\n",
    "    def __init__(self, dataset, token=None):\n",
    "        super().__init__(dataset, token=None)\n",
    "        if convert_encoding != 'OCTMIDI':\n",
    "            self._sizes = np.array([len(self.convert(i)) for i in dataset])\n",
    "        else:\n",
    "            self._sizes = dataset.sizes\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        if convert_encoding != 'OCTMIDI':\n",
    "            item = self.convert(item)\n",
    "        return item\n",
    "\n",
    "    def num_tokens(self, index):\n",
    "        return self._sizes[index].item()\n",
    "\n",
    "    def size(self, index):\n",
    "        return self._sizes[index].item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e233db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairseq.tasks.sentence_prediction.PrependTokenDataset = OctupleTokenDataset\n",
    "fairseq.tasks.masked_lm.PrependTokenDataset = OctupleTokenDataset\n",
    "fairseq.tasks.masked_lm.MaskTokensDataset = OctupleMaskTokensDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d4cdc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d030f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a8169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
